{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPRRaNscuZLL7rDe7iT+BBg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# (3) Python Code:"],"metadata":{"id":"vDTfKKiBVtz5"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2a9vvxqDFfHz","executionInfo":{"status":"ok","timestamp":1713240579650,"user_tz":-540,"elapsed":21224,"user":{"displayName":"최재준","userId":"15020848563771571160"}},"outputId":"4306a06f-b761-44dc-8c70-4c5f9e8fe97e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:01<00:00, 17402961.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 303616.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4422102/4422102 [00:00<00:00, 5472195.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 15036404.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n"]}],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch import nn\n","from torch.utils.data import DataLoader, random_split\n","from torch.optim import SGD, Adam, RMSprop, Adagrad\n","\n","# 데이터셋 설정\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.5,), (0.5,))])\n","\n","train_set = torchvision.datasets.FashionMNIST(root='./data',\n","                                              train=True,\n","                                              download=True,\n","                                              transform=transform)\n","test_set = torchvision.datasets.FashionMNIST(root='./data',\n","                                             train=False,\n","                                             download=True,\n","                                             transform=transform)\n","\n","# 훈련 데이터셋을 훈련 및 검증 세트로 분할\n","num_train = len(train_set)\n","num_val = int(num_train * 0.1)\n","num_train -= num_val\n","train_ds, val_ds = random_split(train_set, [num_train, num_val])\n","\n","train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n","test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n"]},{"cell_type":"code","source":["\n","# 모델 정의\n","class MLP(nn.Module):\n","    def __init__(self, activation_func=nn.ReLU()):\n","        super(MLP, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.layer1 = nn.Linear(28*28, 256)\n","        self.activation = activation_func\n","        self.layer2 = nn.Linear(256, 128)\n","        self.output = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.activation(self.layer1(x))\n","        x = self.activation(self.layer2(x))\n","        x = self.output(x)\n","        return x\n","\n","# 하이퍼파라미터 설정\n","activation_functions = [nn.ReLU(), nn.Tanh(), nn.Softmax(dim=1)]\n","learning_rates = [0.001, 0.01, 0.1]\n","epochs = 30\n","patience = 5\n","\n","# 최적화 함수 설정\n","optimizer_functions = {\n","    'SGD': lambda params, lr: SGD(params, lr=lr),\n","    'Momentum': lambda params, lr: SGD(params, lr=lr, momentum=0.9),\n","    'Adagrad': lambda params, lr: Adagrad(params, lr=lr),\n","    'RMSprop': lambda params, lr: RMSprop(params, lr=lr),\n","    'Adam': lambda params, lr: Adam(params, lr=lr)\n","}\n"],"metadata":{"id":"DIZnwXXVF8N8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 결과 저장을 위한 딕셔너리\n","results = {}\n","\n","for activation_func in activation_functions:\n","    for lr in learning_rates:\n","        for opt_name, opt_func in optimizer_functions.items():\n","            # 모델 초기화\n","            model = MLP(activation_func=activation_func)\n","            optimizer = opt_func(model.parameters(), lr=lr)\n","            loss_fn = nn.CrossEntropyLoss()\n","\n","            best_val_loss = float('inf')\n","            counter = 0\n","\n","            for epoch in range(epochs):\n","                # 훈련 부분\n","                model.train()\n","                for batch, (X, y) in enumerate(train_loader):\n","                    pred = model(X)\n","                    loss = loss_fn(pred, y)\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                # 검증 부분\n","                val_loss = 0.0\n","                model.eval()\n","                with torch.no_grad():\n","                    for X, y in val_loader:\n","                        pred = model(X)\n","                        loss = loss_fn(pred, y)\n","                        val_loss += loss.item()\n","                val_loss /= len(val_loader)\n","\n","                # Early Stopping 체크\n","                if val_loss < best_val_loss:\n","                    best_val_loss = val_loss\n","                    counter = 0\n","                else:\n","                    counter += 1\n","                    if counter >= patience:\n","                        print(f\"Early stopping triggered at epoch {epoch} for activation {activation_func}, learning rate {lr}, optimizer {opt_name}\")\n","                        break\n","\n","            # 최종 평가\n","            correct = 0\n","            total = 0\n","            with torch.no_grad():\n","                for X, y in test_loader:\n","                    pred = model(X)\n","                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","                    total += y.size(0)\n","            accuracy = correct / total\n","            print(f\"Activation: {activation_func}, LR: {lr}, Optimizer: {opt_name}, Accuracy: {accuracy}\")\n","            results[(str(activation_func), lr, opt_name)] = accuracy\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7MM9y5aEF-og","executionInfo":{"status":"ok","timestamp":1713258255009,"user_tz":-540,"elapsed":17655174,"user":{"displayName":"최재준","userId":"15020848563771571160"}},"outputId":"0f89d255-68bf-4f41-c84b-4d64a97a928f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Activation: ReLU(), LR: 0.001, Optimizer: SGD, Accuracy: 0.8222\n","Activation: ReLU(), LR: 0.001, Optimizer: Momentum, Accuracy: 0.874\n","Activation: ReLU(), LR: 0.001, Optimizer: Adagrad, Accuracy: 0.8559\n","Early stopping triggered at epoch 10 for activation ReLU(), learning rate 0.001, optimizer RMSprop\n","Activation: ReLU(), LR: 0.001, Optimizer: RMSprop, Accuracy: 0.8765\n","Early stopping triggered at epoch 15 for activation ReLU(), learning rate 0.001, optimizer Adam\n","Activation: ReLU(), LR: 0.001, Optimizer: Adam, Accuracy: 0.8849\n","Activation: ReLU(), LR: 0.01, Optimizer: SGD, Accuracy: 0.8711\n","Early stopping triggered at epoch 10 for activation ReLU(), learning rate 0.01, optimizer Momentum\n","Activation: ReLU(), LR: 0.01, Optimizer: Momentum, Accuracy: 0.8747\n","Early stopping triggered at epoch 20 for activation ReLU(), learning rate 0.01, optimizer Adagrad\n","Activation: ReLU(), LR: 0.01, Optimizer: Adagrad, Accuracy: 0.8895\n","Early stopping triggered at epoch 15 for activation ReLU(), learning rate 0.01, optimizer RMSprop\n","Activation: ReLU(), LR: 0.01, Optimizer: RMSprop, Accuracy: 0.8343\n","Early stopping triggered at epoch 12 for activation ReLU(), learning rate 0.01, optimizer Adam\n","Activation: ReLU(), LR: 0.01, Optimizer: Adam, Accuracy: 0.8431\n","Early stopping triggered at epoch 16 for activation ReLU(), learning rate 0.1, optimizer SGD\n","Activation: ReLU(), LR: 0.1, Optimizer: SGD, Accuracy: 0.8823\n","Early stopping triggered at epoch 16 for activation ReLU(), learning rate 0.1, optimizer Momentum\n","Activation: ReLU(), LR: 0.1, Optimizer: Momentum, Accuracy: 0.8398\n","Early stopping triggered at epoch 18 for activation ReLU(), learning rate 0.1, optimizer Adagrad\n","Activation: ReLU(), LR: 0.1, Optimizer: Adagrad, Accuracy: 0.8735\n","Early stopping triggered at epoch 7 for activation ReLU(), learning rate 0.1, optimizer RMSprop\n","Activation: ReLU(), LR: 0.1, Optimizer: RMSprop, Accuracy: 0.1012\n","Early stopping triggered at epoch 10 for activation ReLU(), learning rate 0.1, optimizer Adam\n","Activation: ReLU(), LR: 0.1, Optimizer: Adam, Accuracy: 0.1\n","Activation: Tanh(), LR: 0.001, Optimizer: SGD, Accuracy: 0.8188\n","Activation: Tanh(), LR: 0.001, Optimizer: Momentum, Accuracy: 0.8696\n","Activation: Tanh(), LR: 0.001, Optimizer: Adagrad, Accuracy: 0.8539\n","Early stopping triggered at epoch 13 for activation Tanh(), learning rate 0.001, optimizer RMSprop\n","Activation: Tanh(), LR: 0.001, Optimizer: RMSprop, Accuracy: 0.8757\n","Early stopping triggered at epoch 14 for activation Tanh(), learning rate 0.001, optimizer Adam\n","Activation: Tanh(), LR: 0.001, Optimizer: Adam, Accuracy: 0.8745\n","Activation: Tanh(), LR: 0.01, Optimizer: SGD, Accuracy: 0.8716\n","Early stopping triggered at epoch 20 for activation Tanh(), learning rate 0.01, optimizer Momentum\n","Activation: Tanh(), LR: 0.01, Optimizer: Momentum, Accuracy: 0.885\n","Early stopping triggered at epoch 25 for activation Tanh(), learning rate 0.01, optimizer Adagrad\n","Activation: Tanh(), LR: 0.01, Optimizer: Adagrad, Accuracy: 0.8882\n","Early stopping triggered at epoch 19 for activation Tanh(), learning rate 0.01, optimizer RMSprop\n","Activation: Tanh(), LR: 0.01, Optimizer: RMSprop, Accuracy: 0.7414\n","Early stopping triggered at epoch 9 for activation Tanh(), learning rate 0.01, optimizer Adam\n","Activation: Tanh(), LR: 0.01, Optimizer: Adam, Accuracy: 0.7254\n","Early stopping triggered at epoch 22 for activation Tanh(), learning rate 0.1, optimizer SGD\n","Activation: Tanh(), LR: 0.1, Optimizer: SGD, Accuracy: 0.8823\n","Early stopping triggered at epoch 6 for activation Tanh(), learning rate 0.1, optimizer Momentum\n","Activation: Tanh(), LR: 0.1, Optimizer: Momentum, Accuracy: 0.7865\n","Activation: Tanh(), LR: 0.1, Optimizer: Adagrad, Accuracy: 0.8574\n","Early stopping triggered at epoch 8 for activation Tanh(), learning rate 0.1, optimizer RMSprop\n","Activation: Tanh(), LR: 0.1, Optimizer: RMSprop, Accuracy: 0.4492\n","Early stopping triggered at epoch 7 for activation Tanh(), learning rate 0.1, optimizer Adam\n","Activation: Tanh(), LR: 0.1, Optimizer: Adam, Accuracy: 0.1917\n","Early stopping triggered at epoch 11 for activation Softmax(dim=1), learning rate 0.001, optimizer SGD\n","Activation: Softmax(dim=1), LR: 0.001, Optimizer: SGD, Accuracy: 0.1\n","Early stopping triggered at epoch 12 for activation Softmax(dim=1), learning rate 0.001, optimizer Momentum\n","Activation: Softmax(dim=1), LR: 0.001, Optimizer: Momentum, Accuracy: 0.1\n","Activation: Softmax(dim=1), LR: 0.001, Optimizer: Adagrad, Accuracy: 0.6468\n","Activation: Softmax(dim=1), LR: 0.001, Optimizer: RMSprop, Accuracy: 0.8503\n","Activation: Softmax(dim=1), LR: 0.001, Optimizer: Adam, Accuracy: 0.8387\n","Early stopping triggered at epoch 11 for activation Softmax(dim=1), learning rate 0.01, optimizer SGD\n","Activation: Softmax(dim=1), LR: 0.01, Optimizer: SGD, Accuracy: 0.1\n","Early stopping triggered at epoch 6 for activation Softmax(dim=1), learning rate 0.01, optimizer Momentum\n","Activation: Softmax(dim=1), LR: 0.01, Optimizer: Momentum, Accuracy: 0.1\n","Activation: Softmax(dim=1), LR: 0.01, Optimizer: Adagrad, Accuracy: 0.6849\n","Early stopping triggered at epoch 15 for activation Softmax(dim=1), learning rate 0.01, optimizer RMSprop\n","Activation: Softmax(dim=1), LR: 0.01, Optimizer: RMSprop, Accuracy: 0.3875\n","Activation: Softmax(dim=1), LR: 0.01, Optimizer: Adam, Accuracy: 0.7107\n","Early stopping triggered at epoch 6 for activation Softmax(dim=1), learning rate 0.1, optimizer SGD\n","Activation: Softmax(dim=1), LR: 0.1, Optimizer: SGD, Accuracy: 0.1\n","Activation: Softmax(dim=1), LR: 0.1, Optimizer: Momentum, Accuracy: 0.5663\n","Activation: Softmax(dim=1), LR: 0.1, Optimizer: Adagrad, Accuracy: 0.5472\n","Early stopping triggered at epoch 16 for activation Softmax(dim=1), learning rate 0.1, optimizer RMSprop\n","Activation: Softmax(dim=1), LR: 0.1, Optimizer: RMSprop, Accuracy: 0.2881\n","Early stopping triggered at epoch 13 for activation Softmax(dim=1), learning rate 0.1, optimizer Adam\n","Activation: Softmax(dim=1), LR: 0.1, Optimizer: Adam, Accuracy: 0.3582\n"]}]},{"cell_type":"markdown","source":["## 최종결론: 가장 좋은 정확성은 0.8895이며 실험을 통해 획득한 최적 Architecture는 아래와 같다.\n","\n","## 1) Epochs = 20\n","## 2) Activation: ReLU()\n","## 3) LR: 0.01\n","## 4) Optimizer: Adagrad\n"],"metadata":{"id":"MbgCYa9oTdK8"}}]}