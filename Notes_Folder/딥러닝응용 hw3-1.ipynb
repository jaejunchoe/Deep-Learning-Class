{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMaKkrNiF74jvGR+usXA10M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQRwC1GzwNPC","executionInfo":{"status":"ok","timestamp":1713202419389,"user_tz":-540,"elapsed":142003,"user":{"displayName":"최재준","userId":"15020848563771571160"}},"outputId":"609546c2-ed79-48e0-c3aa-30e638a419e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:01<00:00, 14768250.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 272265.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4422102/4422102 [00:00<00:00, 5068988.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 3269080.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Training with optimizer: SGD\n","loss: 2.323056  [    0/60000]\n","loss: 0.587839  [ 6400/60000]\n","loss: 0.721406  [12800/60000]\n","loss: 0.771263  [19200/60000]\n","loss: 0.643121  [25600/60000]\n","loss: 0.389304  [32000/60000]\n","loss: 0.485570  [38400/60000]\n","loss: 0.502587  [44800/60000]\n","loss: 0.505835  [51200/60000]\n","loss: 0.462324  [57600/60000]\n","Testing with optimizer: SGD\n","Test Error: \n"," Accuracy: 81.8%, Avg loss: 0.509029 \n","\n","Training with optimizer: Momentum\n","loss: 0.582544  [    0/60000]\n","loss: 0.645904  [ 6400/60000]\n","loss: 0.568057  [12800/60000]\n","loss: 0.455921  [19200/60000]\n","loss: 0.476197  [25600/60000]\n","loss: 0.385763  [32000/60000]\n","loss: 0.590658  [38400/60000]\n","loss: 0.678371  [44800/60000]\n","loss: 0.705283  [51200/60000]\n","loss: 0.549218  [57600/60000]\n","Testing with optimizer: Momentum\n","Test Error: \n"," Accuracy: 78.0%, Avg loss: 0.621135 \n","\n","Training with optimizer: Adagrad\n","loss: 0.728879  [    0/60000]\n","loss: 1.062733  [ 6400/60000]\n","loss: 0.822430  [12800/60000]\n","loss: 0.841501  [19200/60000]\n","loss: 0.482398  [25600/60000]\n","loss: 1.126716  [32000/60000]\n","loss: 0.733405  [38400/60000]\n","loss: 0.467004  [44800/60000]\n","loss: 0.826106  [51200/60000]\n","loss: 0.578428  [57600/60000]\n","Testing with optimizer: Adagrad\n","Test Error: \n"," Accuracy: 79.4%, Avg loss: 0.566790 \n","\n","Training with optimizer: RMSprop\n","loss: 0.436351  [    0/60000]\n","loss: 1.813119  [ 6400/60000]\n","loss: 2.279129  [12800/60000]\n","loss: 1.905347  [19200/60000]\n","loss: 2.284539  [25600/60000]\n","loss: 2.336472  [32000/60000]\n","loss: 2.318982  [38400/60000]\n","loss: 2.295464  [44800/60000]\n","loss: 2.293641  [51200/60000]\n","loss: 2.317038  [57600/60000]\n","Testing with optimizer: RMSprop\n","Test Error: \n"," Accuracy: 10.0%, Avg loss: 2.400352 \n","\n","Training with optimizer: Adam\n","loss: 2.309713  [    0/60000]\n","loss: 2.304594  [ 6400/60000]\n","loss: 2.302346  [12800/60000]\n","loss: 2.326247  [19200/60000]\n","loss: 2.283797  [25600/60000]\n","loss: 2.301332  [32000/60000]\n","loss: 2.293740  [38400/60000]\n","loss: 2.308130  [44800/60000]\n","loss: 2.321193  [51200/60000]\n","loss: 2.358125  [57600/60000]\n","Testing with optimizer: Adam\n","Test Error: \n"," Accuracy: 10.0%, Avg loss: 2.310794 \n","\n"]}],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.optim import SGD, Adam, RMSprop, Adagrad\n","\n","# 데이터셋 설정\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.5,), (0.5,))])\n","\n","train_set = torchvision.datasets.FashionMNIST(root='./data',\n","                                              train=True,\n","                                              download=True,\n","                                              transform=transform)\n","test_set = torchvision.datasets.FashionMNIST(root='./data',\n","                                             train=False,\n","                                             download=True,\n","                                             transform=transform)\n","\n","train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n","\n","# 모델 정의\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.layer1 = nn.Linear(28*28, 256)\n","        self.relu = nn.ReLU()\n","        self.layer2 = nn.Linear(256, 128)\n","        self.output = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.relu(self.layer1(x))\n","        x = self.relu(self.layer2(x))\n","        x = self.output(x)\n","        return x\n","\n","model = MLP()\n","\n","# 손실 함수 및 최적화 설정\n","loss_fn = nn.CrossEntropyLoss()\n","optimizers = {\n","    'SGD': SGD(model.parameters(), lr=0.1),\n","    'Momentum': SGD(model.parameters(), lr=0.1, momentum=0.9),\n","    'Adagrad': Adagrad(model.parameters(), lr=0.1),\n","    'RMSprop': RMSprop(model.parameters(), lr=0.1),\n","    'Adam': Adam(model.parameters(), lr=0.1)\n","}\n","\n","# 훈련 루프\n","def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    for batch, (X, y) in enumerate(dataloader):\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","# 평가\n","def test(dataloader, model):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss, correct = 0, 0\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","\n","# 훈련 및 평가 실행\n","for name, optimizer in optimizers.items():\n","    print(f\"Training with optimizer: {name}\")\n","    train(train_loader, model, loss_fn, optimizer)\n","    print(f\"Testing with optimizer: {name}\")\n","    test(test_loader, model)\n"]}]}